{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Algorithm from Scratch\n",
    "\n",
    "This notebook implements the **Perceptron learning algorithm** from scratch using only NumPy — no sklearn for the core logic.\n",
    "\n",
    "The Perceptron is one of the earliest supervised learning algorithms. It updates weights iteratively based on misclassified points, making it a great foundation for understanding gradient-based learning.\n",
    "\n",
    "### What this notebook covers\n",
    "- Manual weight update loop (the core perceptron rule)\n",
    "- Training on **linearly separable** data — convergence guaranteed\n",
    "- Training on **linearly inseparable** data — demonstrates the algorithm's fundamental limitation\n",
    "- Visualising the decision boundary and misclassification curve per epoch\n",
    "- Comparing the manual implementation against `sklearn`'s `Perceptron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import make_classification, make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Implementation\n",
    "\n",
    "The perceptron update rule:\n",
    "- If predicted **1** but actual **0**: subtract the feature vector from weights\n",
    "- If predicted **0** but actual **1**: add the feature vector to weights\n",
    "- Correct predictions: no update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X, y, n_epochs=30, initial_weights=None):\n",
    "    \"\"\"\n",
    "    Train a perceptron using the standard weight update rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "    y : ndarray of shape (n_samples,) with binary labels 0 or 1\n",
    "    n_epochs : int — number of full passes through the dataset\n",
    "    initial_weights : ndarray or None — starting weights (zeros if None)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : final weight vector\n",
    "    errors_per_epoch : list of misclassification counts per epoch\n",
    "    \"\"\"\n",
    "    weights = initial_weights.copy() if initial_weights is not None else np.zeros(X.shape[1])\n",
    "    errors_per_epoch = []\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        over_errors = 0    # predicted 1, actual 0\n",
    "        under_errors = 0   # predicted 0, actual 1\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            prediction = np.dot(weights, X[i])\n",
    "\n",
    "            if prediction > 0 and y[i] == 0:\n",
    "                weights -= X[i]\n",
    "                over_errors += 1\n",
    "            elif prediction <= 0 and y[i] == 1:\n",
    "                weights += X[i]\n",
    "                under_errors += 1\n",
    "\n",
    "        errors_per_epoch.append(over_errors + under_errors)\n",
    "\n",
    "    return weights, errors_per_epoch\n",
    "\n",
    "\n",
    "def plot_results(X, y, weights, errors_per_epoch, title_suffix=''):\n",
    "    \"\"\"Plot the decision boundary and training error curve side by side.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Decision boundary\n",
    "    ax1.scatter(X[y == 0, 0], X[y == 0, 1], color='steelblue', label='Class 0', alpha=0.7, edgecolors='k', linewidths=0.4)\n",
    "    ax1.scatter(X[y == 1, 0], X[y == 1, 1], color='tomato', label='Class 1', alpha=0.7, edgecolors='k', linewidths=0.4)\n",
    "\n",
    "    if weights[1] != 0:\n",
    "        x1_vals = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 200)\n",
    "        x2_vals = -(weights[0] / weights[1]) * x1_vals\n",
    "        ax1.plot(x1_vals, x2_vals, color='green', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "    ax1.set_title(f'Decision Boundary {title_suffix}', fontsize=13)\n",
    "    ax1.set_xlabel('Feature 1')\n",
    "    ax1.set_ylabel('Feature 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Training error\n",
    "    ax2.plot(range(len(errors_per_epoch)), errors_per_epoch, marker='o', color='steelblue', linewidth=1.5, markersize=4)\n",
    "    ax2.set_title(f'Misclassifications per Epoch {title_suffix}', fontsize=13)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Misclassifications')\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Final weights: {weights}\")\n",
    "    print(f\"Final misclassifications: {errors_per_epoch[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linearly Separable Data\n",
    "\n",
    "The **Perceptron Convergence Theorem** guarantees that if the classes are linearly separable, the algorithm will find a perfect boundary in a finite number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_sep, y_sep = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, class_sep=2.0, random_state=42\n",
    ")\n",
    "\n",
    "weights_sep, errors_sep = train_perceptron(X_sep, y_sep, n_epochs=30)\n",
    "plot_results(X_sep, y_sep, weights_sep, errors_sep, '(Linearly Separable)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linearly Inseparable Data\n",
    "\n",
    "When classes cannot be separated by a straight line, the perceptron **never converges** — it oscillates indefinitely. This limitation directly motivated the development of multi-layer neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_insep, y_insep = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "weights_insep, errors_insep = train_perceptron(\n",
    "    X_insep, y_insep, n_epochs=100, initial_weights=np.array([1.0, -3.0])\n",
    ")\n",
    "plot_results(X_insep, y_insep, weights_insep, errors_insep, '(Linearly Inseparable)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparison with sklearn's Perceptron\n",
    "\n",
    "We verify our manual implementation by comparing its decision boundary against sklearn's `Perceptron`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_perceptron = Perceptron(max_iter=30, tol=None, random_state=42)\n",
    "sk_perceptron.fit(X_sep, y_sep)\n",
    "\n",
    "sk_weights = sk_perceptron.coef_[0]\n",
    "sk_intercept = sk_perceptron.intercept_[0]\n",
    "\n",
    "print(f\"sklearn accuracy:               {sk_perceptron.score(X_sep, y_sep):.2%}\")\n",
    "print(f\"Manual final misclassifications: {errors_sep[-1]}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X_sep[y_sep == 0, 0], X_sep[y_sep == 0, 1], color='steelblue', label='Class 0', alpha=0.7)\n",
    "ax.scatter(X_sep[y_sep == 1, 0], X_sep[y_sep == 1, 1], color='tomato', label='Class 1', alpha=0.7)\n",
    "\n",
    "x1_vals = np.linspace(X_sep[:, 0].min(), X_sep[:, 0].max(), 200)\n",
    "\n",
    "if weights_sep[1] != 0:\n",
    "    ax.plot(x1_vals, -(weights_sep[0] / weights_sep[1]) * x1_vals,\n",
    "            color='green', linewidth=2, label='Manual Perceptron')\n",
    "\n",
    "if sk_weights[1] != 0:\n",
    "    ax.plot(x1_vals, -(sk_weights[0] * x1_vals + sk_intercept) / sk_weights[1],\n",
    "            color='orange', linewidth=2, linestyle='--', label='sklearn Perceptron')\n",
    "\n",
    "ax.set_title('Manual vs sklearn Decision Boundary', fontsize=13)\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| | Linearly Separable | Linearly Inseparable |\n",
    "|---|---|---|\n",
    "| Convergence | ✅ Guaranteed | ❌ Never converges |\n",
    "| Final errors | 0 | Oscillates |\n",
    "| Decision boundary | Stable | Keeps shifting |\n",
    "\n",
    "The perceptron's failure on non-linearly separable data was a key motivation for multi-layer neural networks and non-linear activation functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
